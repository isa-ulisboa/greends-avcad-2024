{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization of Complex Agro-Environmental Data\n",
    "---\n",
    "## Data transformation\n",
    "\n",
    "### 1. Reshaping Pandas DataFrames by stacking/unstacking and pivoting\n",
    "\n",
    "In many situations, it is important to reshape the data tables to prepare data to be analysed or visualized using certain functions. Here, the most common reshaping operations are demonstrated.\n",
    "\n",
    "#### Create DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'Hold': ['A', 'B', 'C', 'D', 'A', 'C', 'B', 'C', 'A', 'A', 'A', 'B', 'C', 'D', 'C', 'B', 'B', 'C', 'D', 'A'],\n",
    "        'Year': [2001, 2001, 2001, 2001, 2002, 2002, 2002, 2002, 2003, 2003, 2003, 2003, 2004, 2004, 2004, 2004, 2005, 2005, 2005, 2005],\n",
    "        'Fruit': ['strawberry', 'blackberry', 'raspberry', 'blue berry', 'gooseberry', 'strawberry', 'blackberry', 'raspberry', 'blue berry', 'gooseberry', 'strawberry', 'blackberry', 'raspberry', 'blue berry', 'gooseberry', 'strawberry', 'blackberry', 'raspberry', 'blue berry', 'gooseberry'],\n",
    "        'Production': [1000, 300, 400, 500, 800, 1000, 500, 700, 50, 60, 1000, 900, 750, 200, 300, 1000, 900, 250, 750, 50],\n",
    "        }\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stack and Unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack part\n",
    "df2 = df[['Fruit', 'Hold', 'Production']]\n",
    "df2 = df2.set_index(['Hold', 'Fruit'], append=True)\n",
    "df2Unstack = df2.unstack()\n",
    "df2Unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack part ('Hold' as columns)\n",
    "df2 = df[['Fruit', 'Hold', 'Production']]\n",
    "df2 = df2.set_index(['Hold', 'Fruit'], append=True)\n",
    "df2unstack = df2.unstack(level=1) # level=1 means index \"hold\" (last level by default)\n",
    "df2unstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack\n",
    "df2stack = df2unstack.stack()\n",
    "df2stack"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pivot table\n",
    "\n",
    "A more useful and versatile function to unstack a data table is the pandas' pivot_table() function. After normalizing a given dataset, it is frequently useful to pivoting a table in order to convert a categorical variable with n classes into n separate variables. The values may represent different  summarizations of a continuous variable associated to each category, such as the sum, the mean, the maximum, ...\n",
    "\n",
    "Run the examples below using the same DataFrame created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoting based on column 'Fruit'\n",
    "\n",
    "pd.pivot_table(\n",
    "    data=df,\n",
    "    index='Hold', # lines will be indexed by 'Hold'\n",
    "    columns='Fruit', # Columns defined by the 'Fruit' categories\n",
    "    values='Production', # The values in the resulting table\n",
    "    aggfunc='sum' # sums the production by 'Hold'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoting based on 2 columns/variables: 'Fruit' and 'Year'\n",
    "\n",
    "pd.pivot_table(\n",
    "    data=df,\n",
    "    index='Hold',\n",
    "    columns=['Fruit','Year'], # Columns defined by the 'Fruit' and 'Year' categories\n",
    "    values='Production'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivoting based on 'Fruit' using two indices 'hold' and 'Year'\n",
    "\n",
    "pd.pivot_table(\n",
    "    data=df,\n",
    "    index=['Hold', 'Year'],# lines will be indexed by 'Hold' and 'Year'\n",
    "    columns='Fruit',\n",
    "    values='Production'\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Variable standardization and normalization\n",
    "\n",
    "Variable standardization is used in many situations, namely when analyses involve variables measured in different units. It provides methods of rescaling variables without changing the probability distributions of the original data. It is a requirement and a common practice when certain statistical approaches and modelling algorithms are applied to data. For example, in regression-based methods, it allows to directly compare the effect sizes of variables that are measured in very different units; in some multivariate statistical approaches, it avoids giving very varying weights to variables measured in different units; when applying certain modelling algorithms such as Generalized Linar Mixed Models, a previous data standardization is a requirement.\n",
    "\n",
    "The most commonly used standardization method involves centering and scaling operations (Z-score standardization), i.e., each observation  is subtracted by the mean and divided by the standard deviation, so that the new standardized variables has mean = 0 and the standard deviation = 1. After this standardization transformation, the transformed data is often called Z-score.\n",
    "\n",
    "Standardizing variables is quite straightforward in python. Even so, there are some modules that implement variable standardization functions, such as the zscore() of statsmodels or the StandardScaler() function of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sts\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.stats as stm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardize single variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = sts.norm.rvs(scale=50, loc=150, size=1000)\n",
    "sns.histplot(var)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varstd = (var-var.mean())/var.std()\n",
    "sns.histplot(varstd)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 2\n",
    "\n",
    "varstd2 = sts.zscore(var)\n",
    "sns.histplot(varstd2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative 3\n",
    "# Import Z-Score Standard Scaler from the Sklearn package\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale = StandardScaler()\n",
    "var2 = pd.DataFrame(var)\n",
    "varstd3 = scale.fit_transform(var2)\n",
    "sns.histplot(varstd3)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Standardize several predictor variables at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#create data frame\n",
    "df3 = pd.DataFrame({'y': [8, 12, 15, 14, 19, 23, 25, 29],\n",
    "                   'x1': [5, 7, 7, 9, 12, 9, 9, 4],\n",
    "                   'x2': [11, 8, 10, 6, 6, 5, 9, 12],\n",
    "                   'x3': [2, 2, 3, 2, 5, 5, 7, 9]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define predictor variable columns\n",
    "df3_x = df3[['x1', 'x2', 'x3']]\n",
    "\n",
    "# standardize the values for each predictor variable - replace values in df by stadardized ones\n",
    "df3[['x1', 'x2', 'x3']] = (df3_x-df3_x.mean())/df3_x.std()\n",
    "\n",
    "#view new data frame\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view mean of each predictor variable column\n",
    "df3[['x1', 'x2', 'x3']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view standard deviation of each predictor variable column\n",
    "df3[['x1', 'x2', 'x3']].std()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize data using min-max scaler\n",
    "\n",
    "It normalizes the data into a range between 0 and 1 based on the formula: (x - x<sub>min</sub>)/(x<sub>max</sub> - x<sub>min</sub>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define predictor variable columns\n",
    "df4 = df3\n",
    "df4_x = df4[['x1', 'x2', 'x3']]\n",
    "\n",
    "# standardize the values for each predictor variable - replace values in df by stadardized ones\n",
    "df4[['x1', 'x2', 'x3']] = (df4_x-df4_x.min())/(df4_x.max()-df4_x.min())\n",
    "df4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Variable transformation\n",
    "\n",
    "Variable transformation is a very commonly used procedure to prevent some potential statistical problems, especially when using parametric hypothesis testing such as t-test or regression-based methods. It is especially used to tranform variables with very skewed distributions to become more belly shaped. \n",
    "\n",
    "Logarithmic transformation is one of the most commonly used. A more general and complex transformations are the Box-Cox transformations, which involves the estimation of a parameter (lambda) to approximate as much as possible the variable to a normal distribution. For proportions, angular transformations such as the arcsin are more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = sts.expon.rvs(scale=1, loc=0, size=1000)\n",
    "sns.histplot(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic transformation\n",
    "varlog = np.log10(var)\n",
    "sns.histplot(varlog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Square-root transformation\n",
    "varsqrt = np.sqrt(var)\n",
    "sns.histplot(varsqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cube-root transformation\n",
    "varcbrt = np.cbrt(var)\n",
    "sns.histplot(varcbrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box-cox transformation\n",
    "from scipy import stats\n",
    "fitted_data, fitted_lambda = stats.boxcox(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# creating axes to draw plots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    " \n",
    "# plotting the original data(non-normal) and\n",
    "# fitted data (normal)\n",
    "sns.histplot(var, kde = True,\n",
    "            label = \"Original\", ax = ax[0]).legend(loc = \"upper right\")\n",
    " \n",
    "sns.histplot(fitted_data, kde = True,\n",
    "            label = \"Transformed\", ax = ax[1]).legend(loc = \"upper right\")\n",
    "\n",
    "# rescaling the subplots\n",
    "fig.set_figheight(5)\n",
    "fig.set_figwidth(10)\n",
    " \n",
    "print(f\"Lambda value used for Transformation: {fitted_lambda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the values for each predictor variable in a table - replace values in df3 (see above) by transformed ones\n",
    "df3[['x1', 'x2', 'x3']] = np.log10(df3[['x1', 'x2', 'x3']])\n",
    "df3\n",
    "# Alternative:\n",
    "# df3[['x1', 'x2', 'x3']] = df3[['x1', 'x2', 'x3']].apply(np.log10) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Engineer features in the data\n",
    "\n",
    "This kind of data transformation involves generating new variables from existing ones. A common procedure is to aggregate values for each category of a factor, using functions such as the sum, the mean, maximum, ... In other situations, new variables are generated simply by applying a given function (e.g. sum) to a set of columns in a data table.\n",
    "\n",
    "A usefull function to generate new variables is again the pandas' pivot_table(), as shown by the  examples that follows using the df DataFrame generated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sum of each fruit production from df\n",
    "df.pivot_table(index=['Fruit'], values=['Production'], aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sum of each fruit production at each farm hold\n",
    "df.pivot_table(index=['Fruit', 'Hold'], values=['Production'], aggfunc='sum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the median, mean, max and sum of each fruit production per farm hold and year\n",
    "df.pivot_table(index=['Year','Fruit', 'Hold'], values=['Production'], aggfunc={'median', 'mean', 'max', 'sum'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a new variable from summing variables x1, x2 and x3 of df3 generated above.\n",
    "df3['Sum'] = df3[['x1', 'x2', 'x3']].sum(axis=1) # Sums the  variables and adds new variable called 'Sum'\n",
    "df3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Beginner Explanation for Data Transformation, https://towardsdatascience.com/beginner-explanation-for-data-transformation-9add3102f3bf\n",
    "\n",
    "How to Standardize Data in Python (With Examples), https://www.statology.org/standardize-data-python/\n",
    "\n",
    "Normalize a Pandas Column or Dataframe (w/ Pandas or sklearn), https://datagy.io/pandas-normalize-column/\n",
    "\n",
    "Python | Box-Cox Transformation, https://www.geeksforgeeks.org/box-cox-transformation-using-python/\n",
    "\n",
    "Pivot Tables in Pandas with Python, https://datagy.io/python-pivot-tables/\n",
    "\n",
    "Transformations of Stack, Melt, Pivot Table in pandas, https://towardsdatascience.com/transformations-of-stack-melt-pivot-table-901292196d9e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
